{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe2Cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df = pd.read_json('./recipies.json',orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ingredients</th>\n",
       "      <th>cuisine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n",
       "      <td>greek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "      <td>southern_us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "      <td>filipino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[water, vegetable oil, wheat, salt]</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[black pepper, shallots, cornflour, cayenne pe...</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[plain flour, sugar, butter, eggs, fresh ginge...</td>\n",
       "      <td>jamaican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[olive oil, salt, medium shrimp, pepper, garli...</td>\n",
       "      <td>spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[sugar, pistachio nuts, white almond bark, flo...</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[olive oil, purple onion, fresh pineapple, por...</td>\n",
       "      <td>mexican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[chopped tomatoes, fresh basil, garlic, extra-...</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[pimentos, sweet pepper, dried oregano, olive ...</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[low sodium soy sauce, fresh ginger, dry musta...</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[Italian parsley leaves, walnuts, hot red pepp...</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[ground cinnamon, fresh cilantro, chili powder...</td>\n",
       "      <td>mexican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[fresh parmesan cheese, butter, all-purpose fl...</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[tumeric, vegetable stock, tomatoes, garam mas...</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[greek yogurt, lemon curd, confectioners sugar...</td>\n",
       "      <td>british</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[italian seasoning, broiler-fryer chicken, may...</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[sugar, hot chili, asian fish sauce, lime juice]</td>\n",
       "      <td>thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[soy sauce, vegetable oil, red bell pepper, ch...</td>\n",
       "      <td>vietnamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[pork loin, roasted peanuts, chopped cilantro ...</td>\n",
       "      <td>thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[roma tomatoes, kosher salt, purple onion, jal...</td>\n",
       "      <td>mexican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[low-fat mayonnaise, pepper, salt, baking pota...</td>\n",
       "      <td>southern_us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[sesame seeds, red pepper, yellow peppers, wat...</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[marinara sauce, flat leaf parsley, olive oil,...</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[sugar, lo mein noodles, salt, chicken broth, ...</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[herbs, lemon juice, fresh tomatoes, paprika, ...</td>\n",
       "      <td>cajun_creole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[ground black pepper, butter, sliced mushrooms...</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[green bell pepper, egg roll wrappers, sweet a...</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[flour tortillas, cheese, breakfast sausages, ...</td>\n",
       "      <td>mexican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39744</th>\n",
       "      <td>[extra-virgin olive oil, oregano, potatoes, ga...</td>\n",
       "      <td>greek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39745</th>\n",
       "      <td>[quinoa, extra-virgin olive oil, fresh thyme l...</td>\n",
       "      <td>spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39746</th>\n",
       "      <td>[clove, bay leaves, ginger, chopped cilantro, ...</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39747</th>\n",
       "      <td>[water, sugar, grated lemon zest, butter, pitt...</td>\n",
       "      <td>moroccan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39748</th>\n",
       "      <td>[sea salt, pizza doughs, all-purpose flour, co...</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39749</th>\n",
       "      <td>[kosher salt, minced onion, tortilla chips, su...</td>\n",
       "      <td>mexican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39750</th>\n",
       "      <td>[ground black pepper, chicken breasts, salsa, ...</td>\n",
       "      <td>mexican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39751</th>\n",
       "      <td>[olive oil, cayenne pepper, chopped cilantro f...</td>\n",
       "      <td>moroccan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39752</th>\n",
       "      <td>[self rising flour, milk, white sugar, butter,...</td>\n",
       "      <td>southern_us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39753</th>\n",
       "      <td>[rosemary sprigs, lemon zest, garlic cloves, g...</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39754</th>\n",
       "      <td>[jasmine rice, bay leaves, sticky rice, rotiss...</td>\n",
       "      <td>vietnamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39755</th>\n",
       "      <td>[mint leaves, cilantro leaves, ghee, tomatoes,...</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39756</th>\n",
       "      <td>[vegetable oil, cinnamon sticks, water, all-pu...</td>\n",
       "      <td>mexican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39757</th>\n",
       "      <td>[red bell pepper, garlic cloves, extra-virgin ...</td>\n",
       "      <td>greek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39758</th>\n",
       "      <td>[milk, salt, ground cayenne pepper, ground lam...</td>\n",
       "      <td>greek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39759</th>\n",
       "      <td>[red chili peppers, sea salt, onions, water, c...</td>\n",
       "      <td>korean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39760</th>\n",
       "      <td>[butter, large eggs, cornmeal, baking powder, ...</td>\n",
       "      <td>southern_us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39761</th>\n",
       "      <td>[honey, chicken breast halves, cilantro leaves...</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39762</th>\n",
       "      <td>[curry powder, salt, chicken, water, vegetable...</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39763</th>\n",
       "      <td>[fettuccine pasta, low-fat cream cheese, garli...</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39764</th>\n",
       "      <td>[chili powder, worcestershire sauce, celery, r...</td>\n",
       "      <td>mexican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39765</th>\n",
       "      <td>[coconut, unsweetened coconut milk, mint leave...</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39766</th>\n",
       "      <td>[rutabaga, ham, thick-cut bacon, potatoes, fre...</td>\n",
       "      <td>irish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39767</th>\n",
       "      <td>[low-fat sour cream, grated parmesan cheese, s...</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39768</th>\n",
       "      <td>[shredded cheddar cheese, crushed cheese crack...</td>\n",
       "      <td>mexican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39769</th>\n",
       "      <td>[light brown sugar, granulated sugar, butter, ...</td>\n",
       "      <td>irish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39770</th>\n",
       "      <td>[KRAFT Zesty Italian Dressing, purple onion, b...</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39771</th>\n",
       "      <td>[eggs, citrus fruit, raisins, sourdough starte...</td>\n",
       "      <td>irish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39772</th>\n",
       "      <td>[boneless chicken skinless thigh, minced garli...</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39773</th>\n",
       "      <td>[green chile, jalapeno chilies, onions, ground...</td>\n",
       "      <td>mexican</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39774 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             ingredients      cuisine\n",
       "0      [romaine lettuce, black olives, grape tomatoes...        greek\n",
       "1      [plain flour, ground pepper, salt, tomatoes, g...  southern_us\n",
       "2      [eggs, pepper, salt, mayonaise, cooking oil, g...     filipino\n",
       "3                    [water, vegetable oil, wheat, salt]       indian\n",
       "4      [black pepper, shallots, cornflour, cayenne pe...       indian\n",
       "...                                                  ...          ...\n",
       "39769  [light brown sugar, granulated sugar, butter, ...        irish\n",
       "39770  [KRAFT Zesty Italian Dressing, purple onion, b...      italian\n",
       "39771  [eggs, citrus fruit, raisins, sourdough starte...        irish\n",
       "39772  [boneless chicken skinless thigh, minced garli...      chinese\n",
       "39773  [green chile, jalapeno chilies, onions, ground...      mexican\n",
       "\n",
       "[39774 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes_df[['ingredients','cuisine']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df['ingredients_string'] = recipes_df.ingredients.str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/peterschnatz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['pepper',\n",
       " 'salt',\n",
       " 'oil',\n",
       " 'garlic',\n",
       " 'ground',\n",
       " 'fresh',\n",
       " 'sauce',\n",
       " 'sugar',\n",
       " 'onions',\n",
       " 'cheese',\n",
       " 'chicken',\n",
       " 'olive',\n",
       " 'black',\n",
       " 'water',\n",
       " 'red',\n",
       " 'flour',\n",
       " 'tomatoes',\n",
       " 'butter',\n",
       " 'green',\n",
       " 'powder',\n",
       " 'chopped',\n",
       " 'cloves',\n",
       " 'juice',\n",
       " 'white',\n",
       " 'onion',\n",
       " 'eggs',\n",
       " 'rice',\n",
       " 'cream',\n",
       " 'cilantro',\n",
       " 'milk',\n",
       " 'lemon',\n",
       " 'vegetable',\n",
       " 'leaves',\n",
       " 'large',\n",
       " 'ginger',\n",
       " 'corn',\n",
       " 'dried',\n",
       " 'vinegar',\n",
       " 'lime',\n",
       " 'soy',\n",
       " 'cumin',\n",
       " 'all-purpose',\n",
       " 'broth',\n",
       " 'wine',\n",
       " 'chili',\n",
       " 'bell',\n",
       " 'parsley',\n",
       " 'sesame',\n",
       " 'beans',\n",
       " 'grated']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingred_list = recipes_df.ingredients_string.str.cat(sep=' ')\n",
    "#function to split text into word\n",
    "tokens = word_tokenize(ingred_list)\n",
    "vocabulary = set(tokens)\n",
    "print(len(vocabulary))\n",
    "frequency_dist = nltk.FreqDist(tokens)\n",
    "sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sato',\n",
       " 'Jagermeister',\n",
       " 'huckleberries',\n",
       " 'crosswise',\n",
       " 'pike',\n",
       " 'matzos',\n",
       " 'nettle',\n",
       " 'bellpepper',\n",
       " 'Tandoori',\n",
       " 'Naan',\n",
       " 'sucrolose',\n",
       " 'Holland',\n",
       " 'House',\n",
       " 'Wine',\n",
       " 'frankfurters',\n",
       " 'soy-based',\n",
       " 'tortelloni',\n",
       " 'Potato',\n",
       " 'Matzo',\n",
       " 'Meal',\n",
       " 'arctic',\n",
       " 'tonic',\n",
       " 'Chartreuse',\n",
       " 'aloe',\n",
       " 'Honeysuckle',\n",
       " 'White®',\n",
       " 'Turkey',\n",
       " 'cumberland',\n",
       " 'Colman',\n",
       " 'manouri',\n",
       " 'ribeye',\n",
       " 'riblets',\n",
       " 'Plus',\n",
       " 'conchiglie',\n",
       " 'Makers',\n",
       " 'Mark',\n",
       " 'Whisky',\n",
       " 'ranch-style',\n",
       " 'ketjap',\n",
       " 'Daiya',\n",
       " 'Slim',\n",
       " 'Cotto',\n",
       " 'Salami',\n",
       " 'Challenge',\n",
       " 'Brew',\n",
       " 'Size',\n",
       " 'Bags',\n",
       " 'Dip',\n",
       " 'lop',\n",
       " 'chong']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "italian         7838\n",
      "mexican         6438\n",
      "southern_us     4320\n",
      "indian          3003\n",
      "chinese         2673\n",
      "french          2646\n",
      "cajun_creole    1546\n",
      "thai            1539\n",
      "japanese        1423\n",
      "greek           1175\n",
      "spanish          989\n",
      "korean           830\n",
      "vietnamese       825\n",
      "moroccan         821\n",
      "british          804\n",
      "filipino         755\n",
      "irish            667\n",
      "jamaican         526\n",
      "russian          489\n",
      "brazilian        467\n",
      "Name: cuisine, dtype: int64\n",
      "0.19706340825665009\n"
     ]
    }
   ],
   "source": [
    "print(recipes_df.cuisine.nunique())\n",
    "print(recipes_df.cuisine.value_counts())\n",
    "print(recipes_df.cuisine.value_counts().max() / len(recipes_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I blindly just assign all of the recipes to italian, I will get 20% accuracy. What if I randomly select a cuisine and assign it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.051968622718358726\n"
     ]
    }
   ],
   "source": [
    "from  sklearn.metrics  import accuracy_score\n",
    "cuisine_list = list(recipes_df.cuisine.unique())\n",
    "predict_cuisine_random = [random.choice(cuisine_list) for i in range(len(recipes_df))]\n",
    "print(accuracy_score(recipes_df.cuisine,predict_cuisine_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations \n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy for benchmark: 0.10\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.01      0.01      0.01       467\n",
      "     british       0.02      0.02      0.02       804\n",
      "cajun_creole       0.03      0.03      0.03      1546\n",
      "     chinese       0.08      0.08      0.08      2673\n",
      "    filipino       0.02      0.02      0.02       755\n",
      "      french       0.07      0.07      0.07      2646\n",
      "       greek       0.03      0.03      0.03      1175\n",
      "      indian       0.08      0.08      0.08      3003\n",
      "       irish       0.01      0.01      0.01       667\n",
      "     italian       0.20      0.20      0.20      7838\n",
      "    jamaican       0.02      0.02      0.02       526\n",
      "    japanese       0.04      0.04      0.04      1423\n",
      "      korean       0.03      0.03      0.03       830\n",
      "     mexican       0.15      0.15      0.15      6438\n",
      "    moroccan       0.02      0.02      0.02       821\n",
      "     russian       0.02      0.02      0.02       489\n",
      " southern_us       0.11      0.11      0.11      4320\n",
      "     spanish       0.03      0.03      0.03       989\n",
      "        thai       0.04      0.04      0.04      1539\n",
      "  vietnamese       0.01      0.01      0.01       825\n",
      "\n",
      "    accuracy                           0.10     39774\n",
      "   macro avg       0.05      0.05      0.05     39774\n",
      "weighted avg       0.10      0.10      0.10     39774\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc_scores = list()\n",
    "for i in range(1000):\n",
    "    cuisines_perm = np.random.permutation(list(recipes_df.cuisine))\n",
    "    acc_scores.append(accuracy_score(recipes_df.cuisine,cuisines_perm))\n",
    "\n",
    "print('Average accuracy for benchmark: %.2f' %(sum(acc_scores)/len(acc_scores)))\n",
    "print(classification_report(recipes_df.cuisine,cuisines_perm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['greek',\n",
       " 'southern_us',\n",
       " 'filipino',\n",
       " 'indian',\n",
       " 'jamaican',\n",
       " 'spanish',\n",
       " 'italian',\n",
       " 'mexican',\n",
       " 'chinese',\n",
       " 'british',\n",
       " 'thai',\n",
       " 'vietnamese',\n",
       " 'cajun_creole',\n",
       " 'brazilian',\n",
       " 'french',\n",
       " 'japanese',\n",
       " 'irish',\n",
       " 'korean',\n",
       " 'moroccan',\n",
       " 'russian']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(recipes_df.cuisine.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! So it does even worse!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Straight out of the box TF-IDF Naive-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/peterschnatz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [w for w in tokens if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3530"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_no_stops = set(tokens)\n",
    "len(vocab_no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "wordcloud = WordCloud().generate_from_frequencies(frequency_dist)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('WordCloud.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(recipes_df.ingredients_string, recipes_df.cuisine, test_size=0.2,\n",
    "                                                   random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31819, 2894) (7955, 2894)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "train_vectors = vectorizer.fit_transform(X_train)\n",
    "test_vectors = vectorizer.transform(X_test)\n",
    "print(train_vectors.shape, test_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<31819x2894 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 609826 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6696417347580138\n"
     ]
    }
   ],
   "source": [
    "from  sklearn.metrics  import accuracy_score\n",
    "predicted = clf.predict(test_vectors)\n",
    "print(accuracy_score(y_test,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50).fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7426775612822124\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.84      0.42      0.56        90\n",
      "     british       0.60      0.22      0.33       157\n",
      "cajun_creole       0.82      0.66      0.73       313\n",
      "     chinese       0.70      0.88      0.78       518\n",
      "    filipino       0.74      0.51      0.60       158\n",
      "      french       0.58      0.51      0.54       567\n",
      "       greek       0.85      0.65      0.74       222\n",
      "      indian       0.82      0.91      0.87       597\n",
      "       irish       0.64      0.26      0.37       131\n",
      "     italian       0.70      0.91      0.79      1501\n",
      "    jamaican       0.90      0.44      0.59       101\n",
      "    japanese       0.87      0.65      0.74       293\n",
      "      korean       0.87      0.61      0.72       187\n",
      "     mexican       0.84      0.93      0.88      1309\n",
      "    moroccan       0.93      0.58      0.71       170\n",
      "     russian       0.84      0.21      0.33       102\n",
      " southern_us       0.63      0.77      0.69       874\n",
      "     spanish       0.72      0.24      0.36       190\n",
      "        thai       0.75      0.73      0.74       316\n",
      "  vietnamese       0.89      0.45      0.60       159\n",
      "\n",
      "    accuracy                           0.74      7955\n",
      "   macro avg       0.78      0.58      0.63      7955\n",
      "weighted avg       0.75      0.74      0.73      7955\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from  sklearn.metrics  import accuracy_score\n",
    "predicted = clf.predict(test_vectors)\n",
    "print(accuracy_score(y_test,predicted))\n",
    "print(classification_report(y_test,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a pretty good improvement over benchmark!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to spruce it up a bit though. Let's see if we can deal with different ingredients meaning essentially the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the ingredient lists to make sure there are no numbers or non-alphabet characters and everything is lower case. Removing the numbers, i.e. the amount of each ingredient might lose a bit of information. But since the amount of each ingredient is typically not said, the information it gives is minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['romaine lettuce',\n",
       " 'feta cheese crumbles',\n",
       " 'garlic',\n",
       " 'seasoning',\n",
       " 'garbanzo beans',\n",
       " 'black olives',\n",
       " 'purple onion',\n",
       " 'grape tomatoes',\n",
       " 'pepper']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(recipes_df.ingredients[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take out non-alphabet characters, keep spaces and make everything lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingred_lists_no_repeats = list()\n",
    "\n",
    "import re\n",
    "\n",
    "regex = re.compile('[^a-zA-Z_]')\n",
    "#First parameter is the replacement, second parameter is your input string\n",
    "regex.sub('', 'ab3d*E')\n",
    "#Out: 'abdE'\n",
    "\n",
    "for i, row in enumerate(recipes_df['ingredients']):\n",
    "    ingred_list_processed = list()\n",
    "    \n",
    "    for term in row:\n",
    "        temp = regex.sub('',term).lower()\n",
    "        ingred_list_processed.append(temp)\n",
    "\n",
    "    ingred_no_repeats = list(set(ingred_list_processed))\n",
    "    ingred_lists_no_repeats.append(ingred_no_repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df['ingred_processed'] = ingred_lists_no_repeats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if there are repeats ingredient lists which are applied to more than one cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingred_sets = [set(row) for row in list(recipes_df.ingred_processed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df['ingred_set'] = ingred_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_ingred_list = recipes_df.ingred_set.value_counts()[recipes_df.ingred_set.value_counts() > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(repeat_ingred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in repeat_ingred_list.index:\n",
    "    cuisine_set = set(recipes_df.loc[recipes_df.ingred_set == el, 'cuisine'])\n",
    "    if (len(cuisine_set) != 1):\n",
    "        print(el)\n",
    "        print(str(cuisine_set)+'\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find ingredients containing \"tomat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomat_terms = list()\n",
    "for row in recipes_df['ingred_processed']:\n",
    "#     print(row)\n",
    "    term = [ingredient for ingredient in row if 'tomat' in ingredient]\n",
    "    tomat_terms += term\n",
    "\n",
    "tomat_series = pd.Series(tomat_terms).value_counts()[:50]\n",
    "# set(tomat_terms)\n",
    "# words = pd.DataFrame(rows, columns=['book', 'word'])\n",
    "# words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df['ingred_proc_string'] = recipes_df.ingred_processed.str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(recipes_df.ingred_proc_string, recipes_df.cuisine, test_size=0.2,\n",
    "                                                   random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "train_vectors = vectorizer.fit_transform(X_train)\n",
    "test_vectors = vectorizer.transform(X_test)\n",
    "print(train_vectors.shape, test_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.metrics  import accuracy_score\n",
    "y_pred_train = clf.predict(train_vectors)\n",
    "y_pred = clf.predict(test_vectors)\n",
    "\n",
    "print(accuracy_score(y_train,y_pred_train))\n",
    "print(classification_report(y_train,y_pred_train))\n",
    "\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that processing - getting rid of duplicates in the ingredients lists did NOTHING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50).fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.metrics  import accuracy_score\n",
    "y_pred_train = clf.predict(train_vectors)\n",
    "y_pred = clf.predict(test_vectors)\n",
    "\n",
    "print(accuracy_score(y_train,y_pred_train))\n",
    "print(classification_report(y_train,y_pred_train))\n",
    "\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "ing_port_list = list()\n",
    "i=0\n",
    "\n",
    "for row in recipes_df['ingred_proc_string']:\n",
    "    ing_port = porter.stem(row)\n",
    "#     ing_port_list += ing_port\n",
    "    ing_port_list.append(ing_port)\n",
    "    \n",
    "\n",
    "recipes_df['ing_proc_str_port'] = ing_port_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df.drop(columns='ingredients',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df.drop(columns='ingredients_string',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "porter_list = list()\n",
    "i=0\n",
    "\n",
    "for i, row in enumerate(recipes_df['ingred_processed']):\n",
    "    ing_port_list = list()\n",
    "    for term in row:\n",
    "        ing_port = porter.stem(term)\n",
    "        ing_port_list.append(ing_port)\n",
    "    porter_list.append(ing_port_list)\n",
    "\n",
    "\n",
    "print(porter_list)\n",
    "recipes_df['ing_proc_lst_port'] = porter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df.ing_proc_lst_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomat_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "str2Match = tomat_series.index[0]\n",
    "strOptions = list(tomat_series.index)\n",
    "Ratios = process.extract(str2Match,strOptions,limit=50,scorer=fuzz.token_set_ratio)\n",
    "print(Ratios)\n",
    "# You can also select the string with the highest matching percentage\n",
    "highest = process.extractOne(str2Match,strOptions)\n",
    "print(highest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df.ing_proc_str_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df.ing_proc_lst_port[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.token_sort_ratio(recipes_df.ing_proc_lst_port[0][1],\"tomatoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomat_series.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract(\"tomato\",tomat_series.index,scorer=fuzz.token_sort_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df.ingred_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(row):\n",
    "    name = row['ingred_processed']\n",
    "    return process.extractOne(\"tomato\",name,scorer=fuzz.token_sort_ratio)[1]\n",
    "\n",
    "recipes_df[recipes_df.apply(get_ratio, axis=1) > 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomat_terms = list()\n",
    "for row in recipes_df['ing_proc_lst_port']:\n",
    "#     print(row)\n",
    "    term = [ingredient for ingredient in row if 'tomat' in ingredient]\n",
    "    tomat_terms += term\n",
    "\n",
    "tomat_series = pd.Series(tomat_terms).value_counts()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomat_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomat_series.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomat_series pre-porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomat_terms = list()\n",
    "for row in recipes_df['ingred_processed']:\n",
    "#     print(row)\n",
    "    term = [ingredient for ingredient in row if 'tomat' in ingredient]\n",
    "    tomat_terms += term\n",
    "\n",
    "tomat_series = pd.Series(tomat_terms).value_counts()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract(\"tomatoes\",tomat_series.index,scorer=fuzz.token_set_ratio,limit=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(row):\n",
    "    token_scores = list()\n",
    "    name = row['ingred_processed']\n",
    "    for ingredient in name:\n",
    "        token_scores.append(fuzz.token_sort_ratio(ingredient,\"tomatoes\"))\n",
    "\n",
    "    return token_scores\n",
    "\n",
    "for index, row in recipes_df.iterrows():\n",
    "    scores = get_ratio(row)\n",
    "    for score in scores:\n",
    "        if (score > 70):\n",
    "            row['ingred_processed'][scores.index(score)] = \"tomatoes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df.ingred_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df.ingred_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df['new_string'] = recipes_df.ingred_processed.str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(recipes_df.new_string, recipes_df.cuisine, test_size=0.2,\n",
    "                                                   random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "train_vectors = vectorizer.fit_transform(X_train)\n",
    "test_vectors = vectorizer.transform(X_test)\n",
    "print(train_vectors.shape, test_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters={'max_depth':max_depth, 'min_samples_leaf':min_samples_leaf}\n",
    "rf = RandomForestRegressor(n_estimators=20,max_features='sqrt')\n",
    "rf = GridSearchCV(rf, parameters,cv=3)\n",
    "clf = rf.fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.metrics  import accuracy_score\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "predicted = clf.predict(test_vectors)\n",
    "print(accuracy_score(y_test,predicted))\n",
    "print(classification_report(y_test,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def randomforest(max_depth,min_samples_leaf,n_estimators):\n",
    "    parameters={'max_depth':max_depth, 'min_samples_leaf':min_samples_leaf}\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    train_vectors = vectorizer.fit_transform(X_train)\n",
    "    test_vectors = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Create and fit random forest\n",
    "    clf = RandomForestClassifier(n_estimators=30,max_features='sqrt')\n",
    "    clf = GridSearchCV(clf, parameters,cv=3)\n",
    "    clf.fit(train_vectors,y_train)\n",
    "\n",
    "    scores = clf.cv_results_['mean_test_score'].reshape(len(max_depth),len(min_samples_leaf))\n",
    "\n",
    "    # Plot heatmap of R^2 values\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(scores,cmap=plt.cm.inferno)\n",
    "    plt.xlabel('min_samples_leaf')\n",
    "    plt.ylabel('max_depth')\n",
    "    plt.xticks(np.arange(len(min_samples_leaf)),min_samples_leaf)\n",
    "    plt.yticks(np.arange(len(max_depth)),max_depth)\n",
    "    plt.colorbar()\n",
    "\n",
    "    for i in range(len(max_depth)):\n",
    "        for j in range(len(min_samples_leaf)):\n",
    "            text = plt.text(j, i, scores[i, j].round(3),\n",
    "                           horizontalalignment=\"center\",\n",
    "                           verticalalignment=\"center\",\n",
    "                           color=\"g\")\n",
    "\n",
    "    plt.title('Grid Search Results (R^2 score)')\n",
    "    plt.show()\n",
    "\n",
    "    print('Best parameters: ',clf.best_params_)\n",
    "\n",
    "    y_pred_train = clf.predict(train_vectors)\n",
    "    y_pred = clf.predict(test_vectors)\n",
    "\n",
    "    print(accuracy_score(y_train,y_pred_train))\n",
    "    print(classification_report(y_train,y_pred_train))\n",
    "    \n",
    "    print(accuracy_score(y_test,y_pred))\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    \n",
    "    print('\\nFeature Importances')\n",
    "#     importance_frame = pd.DataFrame(list(zip(train_x.columns,clf.best_estimator_.feature_importances_)), columns=['feature','importance'])\n",
    "#     print(importance_frame)\n",
    "    \n",
    "    return clf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = np.floor(np.linspace(4,14,num=4))\n",
    "min_samples_leaf = list(map(int,np.floor(np.logspace(1.0,3.0,num=4))))\n",
    "n_estimators = 30\n",
    "\n",
    "clf_model = randomforest(max_depth,min_samples_leaf,n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = np.floor(np.linspace(12,20,num=4))\n",
    "min_samples_leaf = list(map(int,np.floor(np.logspace(0.2,1.2,num=4))))\n",
    "n_estimators = 30\n",
    "\n",
    "clf_model = randomforest(max_depth,min_samples_leaf,n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "sns.countplot(recipes_df.cuisine)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.rc('axes', labelsize=24)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=24)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=24) \n",
    "plt.tight_layout()\n",
    "fig.savefig('CuisineCount.png', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flour_terms = list()\n",
    "for row in recipes_df['ingred_processed']:\n",
    "#     print(row)\n",
    "    term = [ingredient for ingredient in row if 'flour' in ingredient]\n",
    "    flour_terms += term\n",
    "\n",
    "flour_series = pd.Series(flour_terms).value_counts()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract(\"flour\",flour_series.index,scorer=fuzz.token_sort_ratio,limit=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(row):\n",
    "    token_scores = list()\n",
    "    name = row['ingred_processed']\n",
    "    for ingredient in name:\n",
    "        token_scores.append(fuzz.token_sort_ratio(ingredient,\"flour\"))\n",
    "\n",
    "    return token_scores\n",
    "\n",
    "for index, row in recipes_df.iterrows():\n",
    "    scores = get_ratio(row)\n",
    "    if (max(scores) > 60):\n",
    "        row['ingred_processed'][scores.index(max(scores))] = \"flour\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_scores.index(max(token_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df.ingred_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = recipes_df.iloc[0]\n",
    "\n",
    "token_scores = list()\n",
    "name = row['ingred_processed']\n",
    "for ingredient in name:\n",
    "    token_scores.append(fuzz.token_sort_ratio(ingredient,\"tomatoes\"))\n",
    "print(token_scores)\n",
    "if (max(token_scores) > 70):\n",
    "     row['ingred_processed'][token_scores.index(max(token_scores))] = \"tomatoes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.token_sort_ratio(\"tomatoes\",\"tomatoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[73, 30, 100, 14, 24, 40, 14, 18, 21].index(max([73, 30, 100, 14, 24, 40, 14, 18, 21]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_scores.index(max(token_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row['ingred_processed'][token_scores.index(max(token_scores))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upsampling minority classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(recipes_df.cuisine.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df.cuisine.value_counts()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animal list\n",
    "animal = ['cat', 'dog', 'rabbit', 'guinea pig']\n",
    "\n",
    "# 'rabbit' element is removed\n",
    "animal.remove('rabbit')\n",
    "\n",
    "#Updated Animal List\n",
    "print('Updated animal list: ', animal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old = ['p','m','l']\n",
    "old.remove('l')\n",
    "print(old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of cuisines\n",
    "cuisine_lst = list(recipes_df.cuisine)\n",
    "max_class = recipes_df.cuisine.value_counts().index[0]\n",
    "max_counts = recipes_df.cuisine.value_counts()[0]\n",
    "\n",
    "cuisine_lst.remove(max_class)\n",
    "\n",
    "i_max_class = np.where(recipes_df.cuisine == max_class)[0]\n",
    "\n",
    "for cuisine in cuisine_lst:\n",
    "    # For every observation in class 1, randomly sample from class 0 with replacement\n",
    "    i_cuisine = np.where(recipes_df.cuisine == cuisine)[0]\n",
    "    i_upsamp_cuisine = np.random.choice(i_cuisine, size=max_counts, replace=True)\n",
    "\n",
    "    # Join together class 0's upsampled target vector with class 1's target vector\n",
    "    check = np.concatenate((X_train[i_upsamp_cuisine], X_train[i_max_class]))\n",
    "    break\n",
    "\n",
    "X_train[i_upsamp_cuisine]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train[i_max_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Get list of cuisines\n",
    "cuisine_lst = list(recipes_df.cuisine.unique())\n",
    "max_class = recipes_df.cuisine.value_counts().index[0]\n",
    "max_counts = recipes_df.cuisine.value_counts()[0]\n",
    "\n",
    "cuisine_lst.remove(max_class)\n",
    "\n",
    "i_max_class = np.where(recipes_df.cuisine == max_class)[0]\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_upsampled = recipes_df.iloc[i_max_class]\n",
    "for cuisine in cuisine_lst: \n",
    "    # Upsample minority class\n",
    "    i_cuisine = np.where(recipes_df.cuisine == cuisine)[0]\n",
    "    df_minority_upsampled = resample(recipes_df.iloc[i_cuisine], \n",
    "                                     replace=True,     # sample with replacement\n",
    "                                     n_samples=max_counts,    # to match majority class\n",
    "                                     random_state=123) # reproducible results\n",
    "\n",
    "    # Combine majority class with upsampled minority class\n",
    "    df_upsampled = pd.concat([df_upsampled, df_minority_upsampled])\n",
    "    print(cuisine)\n",
    "    \n",
    "# Display new class counts\n",
    "df_upsampled.cuisine.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "sns.countplot(df_upsampled.cuisine)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.rc('axes', labelsize=24)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=24)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=24) \n",
    "plt.tight_layout()\n",
    "fig.savefig('CuisineCount_upsampled.png', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([X_train, y_train], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Get list of cuisines\n",
    "cuisine_lst = list(result.cuisine.unique())\n",
    "max_class = result.cuisine.value_counts().index[0]\n",
    "max_counts = result.cuisine.value_counts()[0]\n",
    "\n",
    "cuisine_lst.remove(max_class)\n",
    "\n",
    "i_max_class = np.where(result.cuisine == max_class)[0]\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_upsampled = result.iloc[i_max_class]\n",
    "for cuisine in cuisine_lst: \n",
    "    # Upsample minority class\n",
    "    i_cuisine = np.where(result.cuisine == cuisine)[0]\n",
    "    df_minority_upsampled = resample(result.iloc[i_cuisine], \n",
    "                                     replace=True,     # sample with replacement\n",
    "                                     n_samples=max_counts,    # to match majority class\n",
    "                                     random_state=123) # reproducible results\n",
    "\n",
    "    # Combine majority class with upsampled minority class\n",
    "    df_upsampled = pd.concat([df_upsampled, df_minority_upsampled])\n",
    "    print(cuisine)\n",
    "    \n",
    "# Display new class counts\n",
    "df_upsampled.cuisine.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_upsampled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def randomforest(max_depth,min_samples_leaf,n_estimators):\n",
    "    parameters={'max_depth':max_depth, 'min_samples_leaf':min_samples_leaf}\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    train_vectors = vectorizer.fit_transform(df_upsampled.ingred_proc_string)\n",
    "    test_vectors = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Create and fit random forest\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators,max_features='sqrt')\n",
    "    clf = GridSearchCV(clf, parameters,cv=3)\n",
    "    clf.fit(train_vectors,df_upsampled.cuisine)\n",
    "\n",
    "    scores = clf.cv_results_['mean_test_score'].reshape(len(max_depth),len(min_samples_leaf))\n",
    "\n",
    "    # Plot heatmap of R^2 values\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(scores,cmap=plt.cm.inferno)\n",
    "    plt.xlabel('min_samples_leaf')\n",
    "    plt.ylabel('max_depth')\n",
    "    plt.xticks(np.arange(len(min_samples_leaf)),min_samples_leaf)\n",
    "    plt.yticks(np.arange(len(max_depth)),max_depth)\n",
    "    plt.colorbar()\n",
    "\n",
    "    for i in range(len(max_depth)):\n",
    "        for j in range(len(min_samples_leaf)):\n",
    "            text = plt.text(j, i, scores[i, j].round(3),\n",
    "                           horizontalalignment=\"center\",\n",
    "                           verticalalignment=\"center\",\n",
    "                           color=\"g\")\n",
    "\n",
    "    plt.title('Grid Search Results (R^2 score)')\n",
    "    plt.show()\n",
    "\n",
    "    print('Best parameters: ',clf.best_params_)\n",
    "\n",
    "    y_pred_train = clf.predict(train_vectors)\n",
    "    y_pred = clf.predict(test_vectors)\n",
    "\n",
    "    print(accuracy_score(df_upsampled.cuisine,y_pred_train))\n",
    "    print(classification_report(df_upsampled.cuisine,y_pred_train))\n",
    "    \n",
    "    print(accuracy_score(y_test,y_pred))\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    \n",
    "    print('\\nFeature Importances')\n",
    "#     importance_frame = pd.DataFrame(list(zip(train_x.columns,clf.best_estimator_.feature_importances_)), columns=['feature','importance'])\n",
    "#     print(importance_frame)\n",
    "    \n",
    "    return clf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = np.floor(np.linspace(4,14,num=4))\n",
    "min_samples_leaf = list(map(int,np.floor(np.logspace(1.0,3.0,num=4))))\n",
    "n_estimators = 50\n",
    "\n",
    "clf_model = randomforest(max_depth,min_samples_leaf,n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try upsampling on unprocessed ingredient lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(recipes_df.ingredients_string, recipes_df.cuisine, test_size=0.2,\n",
    "                                                   random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "result = pd.concat([X_train, y_train], axis=1, sort=False)\n",
    "\n",
    "# Get list of cuisines\n",
    "cuisine_lst = list(result.cuisine.unique())\n",
    "max_class = result.cuisine.value_counts().index[0]\n",
    "max_counts = result.cuisine.value_counts()[0]\n",
    "\n",
    "cuisine_lst.remove(max_class)\n",
    "\n",
    "i_max_class = np.where(result.cuisine == max_class)[0]\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_upsampled = result.iloc[i_max_class]\n",
    "for cuisine in cuisine_lst: \n",
    "    # Upsample minority class\n",
    "    i_cuisine = np.where(result.cuisine == cuisine)[0]\n",
    "    df_minority_upsampled = resample(result.iloc[i_cuisine], \n",
    "                                     replace=True,     # sample with replacement\n",
    "                                     n_samples=max_counts,    # to match majority class\n",
    "                                     random_state=123) # reproducible results\n",
    "\n",
    "    # Combine majority class with upsampled minority class\n",
    "    df_upsampled = pd.concat([df_upsampled, df_minority_upsampled])\n",
    "    print(cuisine)\n",
    "    \n",
    "# Display new class counts\n",
    "df_upsampled.cuisine.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def randomforest(max_depth,min_samples_leaf,n_estimators):\n",
    "    parameters={'max_depth':max_depth, 'min_samples_leaf':min_samples_leaf}\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    train_vectors = vectorizer.fit_transform(df_upsampled.ingredients_string)\n",
    "    test_vectors = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Create and fit random forest\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators,max_features='sqrt')\n",
    "    clf = GridSearchCV(clf, parameters,cv=3)\n",
    "    clf.fit(train_vectors,df_upsampled.cuisine)\n",
    "\n",
    "    scores = clf.cv_results_['mean_test_score'].reshape(len(max_depth),len(min_samples_leaf))\n",
    "\n",
    "    # Plot heatmap of R^2 values\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(scores,cmap=plt.cm.inferno)\n",
    "    plt.xlabel('min_samples_leaf')\n",
    "    plt.ylabel('max_depth')\n",
    "    plt.xticks(np.arange(len(min_samples_leaf)),min_samples_leaf)\n",
    "    plt.yticks(np.arange(len(max_depth)),max_depth)\n",
    "    plt.colorbar()\n",
    "\n",
    "    for i in range(len(max_depth)):\n",
    "        for j in range(len(min_samples_leaf)):\n",
    "            text = plt.text(j, i, scores[i, j].round(3),\n",
    "                           horizontalalignment=\"center\",\n",
    "                           verticalalignment=\"center\",\n",
    "                           color=\"g\")\n",
    "\n",
    "    plt.title('Grid Search Results (R^2 score)')\n",
    "    plt.show()\n",
    "\n",
    "    print('Best parameters: ',clf.best_params_)\n",
    "\n",
    "    y_pred_train = clf.predict(train_vectors)\n",
    "    y_pred = clf.predict(test_vectors)\n",
    "\n",
    "    print(accuracy_score(df_upsampled.cuisine,y_pred_train))\n",
    "    print(classification_report(df_upsampled.cuisine,y_pred_train))\n",
    "    \n",
    "    print(accuracy_score(y_test,y_pred))\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    \n",
    "    print('\\nFeature Importances')\n",
    "#     importance_frame = pd.DataFrame(list(zip(train_x.columns,clf.best_estimator_.feature_importances_)), columns=['feature','importance'])\n",
    "#     print(importance_frame)\n",
    "    \n",
    "    return clf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = np.floor(np.linspace(4,14,num=4))\n",
    "min_samples_leaf = list(map(int,np.floor(np.logspace(1.0,3.0,num=4))))\n",
    "n_estimators = 20\n",
    "\n",
    "clf_model = randomforest(max_depth,min_samples_leaf,n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for cuisine in list(recipes_df.cuisine.unique()):\n",
    "\n",
    "    words = list(recipes_df.loc[recipes_df.cuisine == cuisine, 'ingredients'])\n",
    "    counter = Counter(words[0])\n",
    "    for i in words[1:]: \n",
    "        counter.update(i)\n",
    "\n",
    "    print(cuisine)\n",
    "    print(counter.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
